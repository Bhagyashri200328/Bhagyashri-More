{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Text Classification\n",
        "Build and compare sentiment analysis models (e.g., movie/product reviews) using sklearn and huggingface transformers.\n",
        "2. Named Entity Recognition (NER)\n",
        "Annotate a small dataset for NER (e.g., extract names, organizations, dates from documents).\n",
        "Fine-tune a pre-trained NER model on this data and evaluate results.\n",
        "3. Text Preprocessing Pipeline\n",
        "Implement and document a pipeline: tokenization, stopword removal, lemmatization/stemming.\n",
        "Compare effects of different preprocessing steps on downstream tasks.\n",
        "4. Keyword/Topic Extraction\n",
        "Use TF-IDF, RAKE, or LDA to extract and summarize main topics/keywords from company data, FAQs, or user queries.\n",
        "5. Text Similarity/Matching\n",
        "Calculate pairwise document similarities (cosine similarity with embeddings or TF-IDF).\n",
        "Build a simple FAQ bot: match a user question to its closest answer from documentation.\n",
        "6. Basic Chatbot/QA System\n",
        "Use RAG or a pre-built Q&A API to answer questions from a specific document or webpage.\n",
        "Evaluate accuracy and suggest improvements.\n",
        "7. Sentiment/Emotion Analysis Pipeline\n",
        "Try simple and advanced approaches (lexicon-based vs huggingface transformers).\n",
        "8. Experiment Tracking\n",
        "Integrate model runs with WandB or MLflow for experiment logging (useful for reproducibility)."
      ],
      "metadata": {
        "id": "GvctJKFkly4d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h0L3OsiIsFNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 .Text Classification**"
      ],
      "metadata": {
        "id": "KukQXdXv65_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I loved this movie, it was amazing!\",\n",
        "        \"Terrible film, I hated it.\",\n",
        "        \"It was an okay movie, not great.\",\n",
        "        \"Best movie ever, fantastic!\",\n",
        "        \"Worst movie I have ever seen.\",\n",
        "        \"This product is excellent, highly recommended!\",\n",
        "        \"Very bad quality, do not buy.\",\n",
        "        \"The product is okay, but could be better.\",\n",
        "        \"Amazing product, I love it!\",\n",
        "        \"Terrible product, waste of money.\"\n",
        "    ],\n",
        "    \"label\": [1,0,1,1,0,1,0,1,1,0] # 0 = Positive , 1 = Negative\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train_vec, y_train)\n",
        "\n",
        "\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "print(\"Sklearn Logistic Regression Results:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXaBoT0IoeUM",
        "outputId": "e987bb55-1330-4c8e-b087-b12130e1c310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sklearn Logistic Regression Results:\n",
            "Accuracy: 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 . Named-Entity Recognition**"
      ],
      "metadata": {
        "id": "oOmvE3RwMKMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch --quiet"
      ],
      "metadata": {
        "id": "v3CJQ9baPxAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify Name,Org. ,Date , Location , etc\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\n",
        "    \"ner\",\n",
        "    model=\"Jean-Baptiste/camembert-ner-with-dates\",\n",
        "    aggregation_strategy=\"simple\"\n",
        ")\n",
        "\n",
        "text = \"Elon Musk founded SpaceX in 2002.\"\n",
        "\n",
        "results = ner(text)\n",
        "\n",
        "print(\"\\nNER Predictions:\")\n",
        "for r in results:\n",
        "    print(r[\"word\"], \"->\", r[\"entity_group\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxZb9jQhMrSi",
        "outputId": "060267e8-7ca1-46ea-b909-91c6c403b5cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NER Predictions:\n",
            "Elon Musk -> PER\n",
            "SpaceX -> ORG\n",
            "in 2002 -> DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **3** . Text Preprocessing"
      ],
      "metadata": {
        "id": "-3GOdU1myZuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "!pip install nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPzKDorHlYPE",
        "outputId": "3ff3f9dc-98da-4352-ee04-7c6245a90a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'NLP stands for Natural Language Processing . it is widey used concept in Deep Learning'"
      ],
      "metadata": {
        "id": "peKcXK9fkzXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokenizer\n",
        "\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o3qeqvnlYMN",
        "outputId": "1c064041-f029-405b-e3f9-ef493fb1fa11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'stands', 'for', 'Natural', 'Language', 'Processing', '.', 'it', 'is', 'widey', 'used', 'concept', 'in', 'Deep', 'Learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenizer\n",
        "\n",
        "sentence_tokens = nltk.sent_tokenize(text)\n",
        "print(sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqFlbYxmmW78",
        "outputId": "69888e22-a740-4244-d0c0-d73de0fdcd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP stands for Natural Language Processing .', 'it is widey used concept in Deep Learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords like in,a,an,the,etc.\n",
        "\n",
        "from os import remove\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "remove_stopwords = [word for word in word_tokens if word not in stop_words]\n",
        "print(remove_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyyIAYcOmW4N",
        "outputId": "7fd7ffda-3d64-48a0-ad73-a408474f20c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'stands', 'Natural', 'Language', 'Processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify Noun fron Text\n",
        "\n",
        "text = 'NLP stands for Natural Language Processing'\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "POS_tags = nltk.pos_tag(word_tokens)\n",
        "NOUN = [word for word, pos in POS_tags if pos.startswith('NN')]\n",
        "print(NOUN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5kn7MB1wCy6",
        "outputId": "79914a8b-a82a-4d37-a982-7b9008e1292f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'Natural', 'Language', 'Processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gives Related Words (Synonyms)\n",
        "\n",
        "wordnet = nltk.corpus.wordnet\n",
        "synonyms = []\n",
        "a=input(\"Enter any One Word/Object Name/etc: \")\n",
        "for syn in wordnet.synsets(a):\n",
        "    for lemma in syn.lemmas():\n",
        "        synonyms.append(lemma.name())\n",
        "print(synonyms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8o4wlP2F3q1",
        "outputId": "11342fe5-7ec5-443b-f43c-c84123904fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter any One Word/Object Name/etc: screen\n",
            "['screen', 'silver_screen', 'projection_screen', 'blind', 'screen', 'screen', 'CRT_screen', 'screen', 'cover', 'covert', 'concealment', 'screen', 'filmdom', 'screenland', 'screen', 'sieve', 'screen', 'screen_door', 'screen', 'screen', 'screen', 'test', 'screen', 'screen', 'screen_out', 'sieve', 'sort', 'screen', 'screen', 'block_out', 'riddle', 'screen', 'shield', 'screen']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify Parts of Speech\n",
        "\n",
        "b = input(\"Enter any Sentence: \")\n",
        "word_tokens_b = nltk.word_tokenize(b)\n",
        "pos_tags = nltk.pos_tag(word_tokens_b)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWh5C075F3j-",
        "outputId": "6ec56bb4-9816-457c-b8d6-79f461c66a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter any Sentence: hello my name is meena\n",
            "[('hello', 'NN'), ('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('meena', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "text = input(\"Enter any Sentence: \")\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"Lemmatized Words: {lemmatized_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmnr82khF3dW",
        "outputId": "ccdba2b5-cb36-4659-d468-edfebfbfa52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter any Sentence: hello everyone my name is meena\n",
            "Original Text: hello everyone my name is meena\n",
            "Lemmatized Words: ['hello', 'everyone', 'my', 'name', 'be', 'meena']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Suffix\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "word =input(\"Enter any Word with Suffix \")\n",
        "lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "print(f\"Lemmatized Word: {lemma}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAicJSUNF3Zw",
        "outputId": "527cde97-7fde-42ab-cab6-be9402a4cbb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter any Word with Suffix running\n",
            "Lemmatized Word: run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYUypHoaa-b4",
        "outputId": "587b6d1b-e6df-4baf-ad36-ba2a6567ed20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Prefix and Suffix\n",
        "\n",
        "a = input(\"Enter any Word: \")\n",
        "\n",
        "prefixes = {'dis','diss','pre','sub','un'}\n",
        "suffixes = {'ing','ed','fully','ly','full','ful'}\n",
        "\n",
        "processed_word = a\n",
        "\n",
        "for p in prefixes:\n",
        "    if processed_word.startswith(p):\n",
        "        processed_word = processed_word[len(p):]\n",
        "        break\n",
        "\n",
        "for s in suffixes:\n",
        "    if processed_word.endswith(s):\n",
        "        processed_word = processed_word[:-len(s)]\n",
        "        break\n",
        "\n",
        "print(processed_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfw6IiRnF3QX",
        "outputId": "e6aac288-4e70-42c7-c729-819529b18773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter any Word: flying\n",
            "fly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuation\n",
        "\n",
        "a= input(\"Enter any Sentence: \")\n",
        "\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "no_punct = \"\"\n",
        "for char in a:\n",
        "    if char not in punctuations:\n",
        "        no_punct = no_punct + char\n",
        "print(no_punct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agfc_aDSbAJ4",
        "outputId": "a5d4b19a-f034-425b-d6c6-c0f815bab99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:5: SyntaxWarning: invalid escape sequence '\\,'\n",
            "<>:5: SyntaxWarning: invalid escape sequence '\\,'\n",
            "/tmp/ipython-input-2792966998.py:5: SyntaxWarning: invalid escape sequence '\\,'\n",
            "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter any Sentence: hello......!!!!!!!!\n",
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4 . Keyword/Topic Extraction(TF-IDF, RAKE , LDA)**"
      ],
      "metadata": {
        "id": "aZL29xzx30ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn nltk rake-nltk gensim --quiet"
      ],
      "metadata": {
        "id": "AIZ-MBNk3d0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rake_nltk import Rake\n",
        "from gensim import corpora, models\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "documents = [\n",
        "    \"Our company provides cloud-based storage solutions for small businesses.\",\n",
        "    \"Users can reset their passwords and manage their accounts online.\",\n",
        "    \"We offer 24/7 customer support and technical assistance.\",\n",
        "    \"Our team helps clients migrate their data to the cloud securely.\"\n",
        "]\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# TF-IDF Keywords\n",
        "\n",
        "print(\"==== TF-IDF Keywords ====\")\n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(documents)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    scores = zip(feature_names, X[i].toarray()[0])\n",
        "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)[:5]  # top 5 keywords\n",
        "    keywords = [word for word, score in sorted_scores if score > 0]\n",
        "    print(f\"Document {i+1} TF-IDF Keywords: {keywords}\")\n",
        "\n",
        "# RAKE Keywords\n",
        "\n",
        "print(\"\\n==== RAKE Keywords ====\")\n",
        "r = Rake(stopwords=stop_words, min_length=1, max_length=3)\n",
        "for i, doc in enumerate(documents):\n",
        "    r.extract_keywords_from_text(doc)\n",
        "    keywords = r.get_ranked_phrases()[:5]  # top 5 phrases\n",
        "    print(f\"Document {i+1} RAKE Keywords: {keywords}\")\n",
        "\n",
        "# LDA Topics\n",
        "\n",
        "print(\"\\n==== LDA Topics ====\")\n",
        "\n",
        "texts = [[word.lower() for word in word_tokenize(doc) if word.isalpha() and word.lower() not in stop_words]\n",
        "         for doc in documents]\n",
        "\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "\n",
        "lda_model = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15, random_state=42)\n",
        "\n",
        "for idx, topic in lda_model.print_topics(num_words=5):\n",
        "    print(f\"Topic {idx+1}: {topic}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLgjn9s-cJWM",
        "outputId": "8eed3594-d875-4f68-8a37-09c1f7ad33da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== TF-IDF Keywords ====\n",
            "Document 1 TF-IDF Keywords: ['based', 'based storage', 'businesses', 'cloud based', 'company']\n",
            "Document 2 TF-IDF Keywords: ['accounts', 'accounts online', 'manage', 'manage accounts', 'online']\n",
            "Document 3 TF-IDF Keywords: ['24', '24 customer', 'assistance', 'customer', 'customer support']\n",
            "Document 4 TF-IDF Keywords: ['clients', 'clients migrate', 'cloud securely', 'data', 'data cloud']\n",
            "\n",
            "==== RAKE Keywords ====\n",
            "Document 1 RAKE Keywords: ['company provides cloud', 'based storage solutions', 'small businesses']\n",
            "Document 2 RAKE Keywords: ['accounts online', 'users', 'reset', 'passwords', 'manage']\n",
            "Document 3 RAKE Keywords: ['7 customer support', 'technical assistance', 'offer 24']\n",
            "Document 4 RAKE Keywords: ['cloud securely', 'data']\n",
            "\n",
            "==== LDA Topics ====\n",
            "Topic 1: 0.065*\"solutions\" + 0.065*\"small\" + 0.065*\"provides\" + 0.065*\"businesses\" + 0.065*\"storage\"\n",
            "Topic 2: 0.060*\"migrate\" + 0.060*\"clients\" + 0.060*\"securely\" + 0.060*\"data\" + 0.060*\"cloud\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5**. Text Similarity Matching"
      ],
      "metadata": {
        "id": "RRsoethw5VF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask the listed Ques\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "faq_questions = [\n",
        "    \"How do I reset my password?\",\n",
        "    \"What is the refund policy?\",\n",
        "    \"How can I contact support?\",\n",
        "    \"Where can I download the app?\",\n",
        "    \"How do I change my email address?\"\n",
        "]\n",
        "\n",
        "faq_answers = [\n",
        "    \"To reset your password, go to settings > account > reset password.\",\n",
        "    \"Our refund policy allows refunds within 30 days of purchase.\",\n",
        "    \"You can contact support via email at support@example.com.\",\n",
        "    \"You can download the app from the App Store or Google Play.\",\n",
        "    \"To change your email address, go to your profile settings and update your email.\"\n",
        "]\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "faq_vectors = vectorizer.fit_transform(faq_questions)\n",
        "\n",
        "print(\" FAQ Bot is ready! Type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_question = input(\"You: \")\n",
        "    if user_question.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Bot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "\n",
        "    user_vector = vectorizer.transform([user_question])\n",
        "\n",
        "\n",
        "    similarities = cosine_similarity(user_vector, faq_vectors)\n",
        "    best_idx = similarities.argmax()\n",
        "\n",
        "\n",
        "    print(\"Bot:\", faq_answers[best_idx], \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UZZPH1sbYiM",
        "outputId": "89a61b74-1538-495f-e7b1-73f0d4afd03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " FAQ Bot is ready! Type 'exit' to quit.\n",
            "\n",
            "You: how can i contact support ?\n",
            "Bot: You can contact support via email at support@example.com. \n",
            "\n",
            "You: exit\n",
            "Bot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Basic Chatbot/QA System**"
      ],
      "metadata": {
        "id": "f8XqJZOz5IG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask Que from the document_text\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "document_text = \"\"\"\n",
        "Plants grow using sunlight, water, and nutrients.\n",
        "Sunlight allows photosynthesis, which provides energy.\n",
        "Water transports nutrients throughout the plant.\n",
        "Nutrients build plant tissues and help the plant grow.\n",
        "\"\"\"\n",
        "\n",
        "sentences = [s.strip() for s in document_text.split(\".\") if s.strip()]\n",
        "\n",
        "\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "sentence_embeddings = embedder.encode(sentences)\n",
        "\n",
        "\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=150,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "print(\" Offline QA Assistant (type 'exit' to quit)\\n\")\n",
        "\n",
        "while True:\n",
        "    question = input(\"You: \")\n",
        "    if question.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Assistant: Goodbye!\")\n",
        "        break\n",
        "\n",
        "\n",
        "    q_embed = embedder.encode([question])\n",
        "    similarities = cosine_similarity(q_embed, sentence_embeddings)[0]\n",
        "    top_idx = similarities.argsort()[-2:][::-1]\n",
        "    context = \" \".join(sentences[i] for i in top_idx)\n",
        "\n",
        "\n",
        "    prompt = f\"Use the following information to answer the question naturally:\\n{context}\\nQuestion: {question}\\nAnswer:\"\n",
        "    response = generator(prompt, num_return_sequences=1)[0][\"generated_text\"]\n",
        "    answer_text = response.replace(prompt, \"\").strip()\n",
        "\n",
        "    print(\"Assistant:\", answer_text, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezjf4EbfbYnt",
        "outputId": "fcf23e0c-504c-4a7c-dd13-edc36a51ffc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Offline QA Assistant (type 'exit' to quit)\n",
            "\n",
            "You: what nutrients does ?\n",
            "Assistant: Nutrients are a fundamental part of the plant's body and \n",
            "\n",
            "You: exit\n",
            "Assistant: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7 .Sentiment/Emotion Analysis**"
      ],
      "metadata": {
        "id": "CkqJAX4OVnw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk --quiet\n",
        "!pip install transformers torch --quiet\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG5KQpgGlqgZ",
        "outputId": "0180a830-d559-451a-f9f2-8bfd7e272589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I drink juice everyday\"\n",
        "\n",
        "# Lexicon-Based Sentiment\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "score = sia.polarity_scores(text)\n",
        "print(\"\\nLexicon-Based Sentiment:\")\n",
        "print(score)\n",
        "\n",
        "# Transformer Sentiment\n",
        "\n",
        "sentiment_model = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "result = sentiment_model(text)\n",
        "print(\"\\nTransformer Sentiment:\")\n",
        "print(result)\n",
        "\n",
        "# Emotion Score\n",
        "\n",
        "emotion_model = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "    return_all_scores=True\n",
        ")\n",
        "\n",
        "results = emotion_model(text)\n",
        "\n",
        "print(\"\\nEmotion Scores:\")\n",
        "for r in results[0]:\n",
        "    print(r[\"label\"], \"->\", round(r[\"score\"], 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioG0p5WwgkcP",
        "outputId": "789a7028-a046-42a2-dde1-9e553a6f8acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lexicon-Based Sentiment:\n",
            "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
            "\n",
            "Transformer Sentiment:\n",
            "[{'label': 'POSITIVE', 'score': 0.9973875880241394}]\n",
            "\n",
            "Emotion Scores:\n",
            "anger -> 0.039\n",
            "disgust -> 0.131\n",
            "fear -> 0.014\n",
            "joy -> 0.014\n",
            "neutral -> 0.761\n",
            "sadness -> 0.025\n",
            "surprise -> 0.015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SPzU1m5riZpW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}